batch_size: 32 #6464                         # batch size 256 for hiv muv
warm_up: 10                             # warm-up epochs
epochs: 100                             # total number of epochs

load_model: None                        # resume training None pubchem_pretrained_gin
eval_every_n_epochs: 1                  # validation frequency
save_every_n_epochs: 200                  # automatic model saving frequecy
log_every_n_steps: 50                   # print training log frequency

fp16_precision: False                   # float precision 16 (i.e. True/False)
init_lr: 0.005                         # initial learning rate for Adam 0.0001  
weight_decay: 1e-5                      # weight decay for Adam
gpu: cuda:4                             # training GPU 

model_type: gin                         # GNN backbone (i.e., gin/gcn)
model: 
  num_layer: 5                          # number of graph conv layers
  emb_dim: 300                          # embedding dimension in graph conv layers
  feat_dim: 512                         # output feature dimention
  drop_ratio: 0                         # dropout ratio
  pool: mean                            # readout pooling (i.e., mean/max/add)

dataset_name:  freesolv #bbbp bace sider freesolv esol lipo qm7 
dataset:
  num_workers: 12                       # dataloader number of workers
  valid_size: 0.1                      # ratio of validation data
  data_path:  data/freesolv/freesolv.csv # path of pre-training data data/pubchem-10m-clean.txt bbbp/BBBP.csv clintox/clintox.csv bace/bace.csv hiv/HIV.csv sider

loss:
  temperature: 0.1                      # temperature of NT-Xent loss
  use_cosine_similarity: True           # whether to use cosine similarity in NT-Xent loss (i.e. True/False)


llm: mistral #llama2 gpt mistral
